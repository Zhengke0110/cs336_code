import torch
import torch.nn.functional as F
from torch import nn
from typing import List, Tuple, Dict, Set
import collections
import os
import re
import json
from tqdm import tqdm


def bytes_to_unicode():
    """æŠŠ256ä¸ªå­—èŠ‚æ˜ å°„åˆ°Unicodeå­—ç¬¦ä¸Šï¼Œç¡®ä¿æ¯ä¸ªå­—èŠ‚éƒ½æœ‰å¯¹åº”çš„å¯æ‰“å°å­—ç¬¦"""
    # é€‰å‡ºæœ¬èº«å°±å¯ä»¥æ‰“å°çš„å­—èŠ‚ï¼ˆ188ä¸ªï¼‰
    bs = (
        list(range(ord("!"), ord("~") + 1))
        + list(range(ord("Â¡"), ord("Â¬") + 1))
        + list(range(ord("Â®"), ord("Ã¿") + 1))
    )
    # å¤åˆ¶ä¸€ä»½æ•°ç»„
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:  # å¦‚æœè¿™ä¸ªå­—èŠ‚ä¸å¯è¢«æ‰“å°
            bs.append(b)
            cs.append(2**8 + n)  # æ˜ å°„åˆ°256+nçš„ä½ç½®çš„Unicodeå­—ç¬¦
            n += 1
    cs = [chr(n) for n in cs]  # å°†Unicodeç ä½è½¬ä½å®é™…å­—ç¬¦
    return dict(zip(bs, cs))  # è¿”å›å­—èŠ‚-å­—ç¬¦çš„æ˜ å°„è¡¨


def get_stats(token_sequences: List[List[str]]) -> collections.Counter:
    """è®¡ç®—æ‰€æœ‰ç›¸é‚»ç¬¦å·å¯¹çš„é¢‘ç‡"""
    pairs = collections.Counter()  # éœ€è¦åŠ æ‹¬å·æ¥å®ä¾‹åŒ–

    for token in token_sequences:
        for i in range(len(token) - 1):
            pair = (token[i], token[i + 1])
            pairs[pair] += 1
    return pairs


def merge(
    sequences: List[List[str]],
    pair: Tuple[str, str],
    new_token: str,
) -> List[List[str]]:
    """å°†æ‰€æœ‰åºåˆ—ä¸­çš„æŒ‡å®štokenå¯¹åˆå¹¶ä¸ºæ–°token"""
    result = []
    (t1, t2) = pair
    for seq in sequences:
        new_seq = []
        i = 0
        while i < len(seq):
            # å¦‚æœå½“å‰ä½ç½®å’Œä¸‹ä¸€ä½ç½®åŒ¹é…pairï¼Œåˆ™åˆå¹¶
            if i < len(seq) - 1 and seq[i] == t1 and seq[i + 1] == t2:
                new_seq.append(new_token)
                i += 2  # è·³è¿‡å·²åˆå¹¶çš„ä¸¤ä¸ªtoken
            else:
                new_seq.append(seq[i])
                i += 1
        result.append(new_seq)
    return result


def train_bpe(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: list[str],
):
    """è®­ç»ƒBPE tokenizerï¼Œè¿”å›è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™"""
    # å»ºç«‹å­—èŠ‚<->Unicodeå­—ç¬¦çš„æ˜ å°„
    byte_to_unicode = bytes_to_unicode()
    unicode_to_bytes = {v: bytes([k]) for k, v in byte_to_unicode.items()}

    if not isinstance(vocab_size, int) or vocab_size <= 0:
        raise ValueError("vocab_size must > 0")

    # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼š256ä¸ªåŸºç¡€å­—èŠ‚
    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    next_id: int = 256
    existing_bytes: Set[bytes] = set(vocab.values())

    # æ·»åŠ ç‰¹æ®Štokenåˆ°è¯æ±‡è¡¨
    for token in special_tokens:
        if len(vocab) >= vocab_size:
            break
        token_bytes = token.encode("utf-8")
        if token_bytes not in existing_bytes:
            vocab[next_id] = token_bytes
            existing_bytes.add(token_bytes)
            next_id += 1

    # è¯»å–è®­ç»ƒæ–‡æœ¬
    print(f"ğŸ“– è¯»å–è®­ç»ƒæ–‡ä»¶: {input_path}")
    try:
        with open(input_path, "r", encoding="utf-8", errors="ignore") as f:
            text = f.read()
    except FileNotFoundError:
        text = ""

    print(f"âœ“ æ–‡ä»¶å¤§å°: {len(text):,} å­—ç¬¦")

    # æŒ‰å•è¯åˆ†å‰²æ–‡æœ¬ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
    print("ğŸ” åˆ†è¯å¤„ç†ä¸­...")
    words: List[str] = re.findall(r"\s*\S+", text)
    print(f"âœ“ å…±æ‰¾åˆ° {len(words):,} ä¸ªè¯")

    # å°†æ¯ä¸ªå•è¯è½¬æ¢ä¸ºUnicodeå­—ç¬¦åºåˆ—
    print("ğŸ”„ è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—...")
    sequences: List[List[str]] = []
    for word in tqdm(words, desc="å¤„ç†å•è¯", unit="è¯", ncols=80):
        word_bytes: bytes = word.encode("utf-8")
        if not word_bytes:
            continue
        sequences.append([byte_to_unicode[b] for b in word_bytes])

    merges: List[Tuple[bytes, bytes]] = []

    # è®¡ç®—éœ€è¦çš„mergeæ¬¡æ•°
    num_merges = vocab_size - len(vocab)
    print(f"\nğŸš€ å¼€å§‹BPEè®­ç»ƒ")
    print(f"   åˆå§‹è¯æ±‡è¡¨: {len(vocab)}")
    print(f"   ç›®æ ‡è¯æ±‡è¡¨: {vocab_size}")
    print(f"   éœ€è¦åˆå¹¶: {num_merges} æ¬¡\n")

    # è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„tokenå¯¹
    pbar = tqdm(total=num_merges, desc="è®­ç»ƒè¿›åº¦", unit="merge", ncols=100)

    while len(vocab) < vocab_size:
        if not sequences:
            break

        # ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»tokenå¯¹çš„é¢‘ç‡
        pair_counts = get_stats(sequences)
        if not pair_counts:
            break

        # æ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„tokenå¯¹
        best_pair: Tuple[str, str] = max(pair_counts, key=lambda x: pair_counts[x])
        freq = pair_counts[best_pair]

        # åˆ›å»ºæ–°tokenï¼ˆæ‹¼æ¥ä¸¤ä¸ªUnicodeå­—ç¬¦ï¼‰
        new_token: str = best_pair[0] + best_pair[1]

        # è½¬æ¢ä¸ºå­—èŠ‚è¡¨ç¤º
        b1 = unicode_to_bytes[best_pair[0]]
        b2 = unicode_to_bytes[best_pair[1]]
        new_bytes: bytes = b1 + b2

        # æ›´æ–°æ˜ å°„å’Œè¯æ±‡è¡¨
        unicode_to_bytes[new_token] = new_bytes
        vocab[next_id] = new_bytes
        merges.append((b1, b2))

        # åœ¨æ‰€æœ‰åºåˆ—ä¸­åº”ç”¨è¿™æ¬¡åˆå¹¶
        sequences = merge(sequences, best_pair, new_token)

        # æ›´æ–°è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå½“å‰åˆå¹¶çš„tokenä¿¡æ¯
        try:
            token_display = new_bytes.decode("utf-8", errors="replace")[:20]
        except:
            token_display = str(new_bytes)[:20]
        pbar.set_postfix({"token": token_display, "freq": f"{freq:,}"})
        pbar.update(1)

        next_id += 1

    pbar.close()

    # ä¿å­˜è¯æ±‡è¡¨åˆ°JSON
    print("\nğŸ’¾ ä¿å­˜è¯æ±‡è¡¨åˆ° vocab.json...")
    with open("vocab.json", "w", encoding="utf-8") as f:
        vocab_dict = {
            token_id: token_bytes.decode("utf-8", errors="replace")
            for token_id, token_bytes in vocab.items()
        }
        json.dump(vocab_dict, f, ensure_ascii=False, indent=4)

    # ä¿å­˜åˆå¹¶è§„åˆ™åˆ°æ–‡æœ¬æ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜åˆå¹¶è§„åˆ™åˆ° merges.txt...")
    with open("merges.txt", "w", encoding="utf-8") as f:
        for b1, b2 in merges:
            s1 = b1.decode("utf-8", errors="replace")
            s2 = b2.decode("utf-8", errors="replace")
            f.write(f"{s1} {s2}\n")

    print(f"\nâœ¨ è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    print(f"   åˆå¹¶æ“ä½œæ¬¡æ•°: {len(merges)}")

    return vocab, merges


if __name__ == "__main__":
    special_tokens = ["<|endoftext|>"]
    vocab, merges = train_bpe("./data/TinyStories-valid.txt", 20000, special_tokens)

    print(vocab[:10])
