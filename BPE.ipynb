{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5feb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a924c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BPETokenizerParms:\n",
    "    vocab: dict[int, bytes] # index -> bytes\n",
    "    merges: dict[tuple[int, int], int] # index1, index2 -> new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7257b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(ABC):\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        # 检查当前位置和下一位置是否匹配pair\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)  # 用新索引替换pair\n",
    "            i += 2  # 跳过已合并的两个token\n",
    "        else:\n",
    "            new_indices.append(indices[i])  # 保留原token\n",
    "            i += 1\n",
    "    return new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78250adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, params: BPETokenizerParms):\n",
    "        self.params = params\n",
    "\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        \"\"\"将字符串编码为token索引列表\"\"\"\n",
    "        # 1. 先将字符串转换为UTF-8字节序列\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "\n",
    "        # 2. 按照训练时学到的合并规则，依次合并token\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        \"\"\"将token索引列表解码为字符串\"\"\"\n",
    "        # 1. 根据vocab将每个索引映射回对应的字节序列\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        # 2. 拼接所有字节并解码为UTF-8字符串\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParms:\n",
    "    # 1. 初始化：将字符串转为UTF-8字节序列（每个字节作为一个token）\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}  # 记录合并规则\n",
    "    vocab: dict[int, bytes] = {\n",
    "        x: bytes([x]) for x in range(256)\n",
    "    }  # 初始词汇表包含所有256个字节\n",
    "\n",
    "    # 2. 执行num_merges次合并操作\n",
    "    for i in range(num_merges):\n",
    "        # 统计所有相邻token对的出现频率\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):\n",
    "            counts[(index1, index2)] += 1\n",
    "\n",
    "        # 找到出现次数最多的token对\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "\n",
    "        # 为这个token对分配新的索引（从256开始递增）\n",
    "        new_index = 256 + i\n",
    "        merges[pair] = new_index  # 记录合并规则\n",
    "\n",
    "        # 新token = 两个旧token的字节拼接\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "\n",
    "        # 在当前序列中应用这个合并规则\n",
    "        indices = merge(indices=indices, pair=pair, new_index=new_index)\n",
    "\n",
    "    return BPETokenizerParms(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84da23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练字符串: the cat in the hat\n",
      "原始字节: [116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116]\n",
      "\n",
      "训练后的词汇表大小: 259\n",
      "合并规则数量: 3\n",
      "\n",
      "合并规则详情:\n",
      "  (116, 104) -> 256: b't' + b'h' = b'th' ('th')\n",
      "  (256, 101) -> 257: b'th' + b'e' = b'the' ('the')\n",
      "  (257, 32) -> 258: b'the' + b' ' = b'the ' ('the ')\n",
      "\n",
      "==================================================\n",
      "测试字符串: the quick brown fox\n",
      "原始字节: [116, 104, 101, 32, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]\n",
      "编码后的indices: [258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]\n",
      "编码后token详情: 'the ' | 'q' | 'u' | 'i' | 'c' | 'k' | ' ' | 'b' | 'r' | 'o' | 'w' | 'n' | ' ' | 'f' | 'o' | 'x'\n",
      "\n",
      "解码后的字符串: the quick brown fox\n",
      "断言通过: True\n"
     ]
    }
   ],
   "source": [
    "def bpe_tokenizer():\n",
    "    string = \"the cat in the hat\"\n",
    "    print(f\"训练字符串: {string}\")\n",
    "    print(f\"原始字节: {list(string.encode('utf-8'))}\")\n",
    "\n",
    "    params = train_bpe(string, num_merges=3)\n",
    "    print(f\"\\n训练后的词汇表大小: {len(params.vocab)}\")\n",
    "    print(f\"合并规则数量: {len(params.merges)}\")\n",
    "    print(f\"\\n合并规则详情:\")\n",
    "    for pair, new_idx in params.merges.items():\n",
    "        idx1, idx2 = pair\n",
    "        token1 = params.vocab[idx1]\n",
    "        token2 = params.vocab[idx2]\n",
    "        new_token = params.vocab[new_idx]\n",
    "        print(\n",
    "            f\"  {pair} -> {new_idx}: {token1} + {token2} = {new_token} ('{new_token.decode('utf-8', errors='ignore')}')\"\n",
    "        )\n",
    "\n",
    "    tokenizer = BPETokenizer(params)\n",
    "\n",
    "    string = \"the quick brown fox\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试字符串: {string}\")\n",
    "    print(f\"原始字节: {list(string.encode('utf-8'))}\")\n",
    "\n",
    "    indices = tokenizer.encode(string)\n",
    "    print(f\"编码后的indices: {indices}\")\n",
    "    print(f\"编码后token详情: \", end=\"\")\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"'{params.vocab[idx].decode('utf-8', errors='ignore')}'\"\n",
    "                for idx in indices\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    reconstructed_string = tokenizer.decode(indices)\n",
    "    print(f\"\\n解码后的字符串: {reconstructed_string}\")\n",
    "    print(f\"断言通过: {string == reconstructed_string}\")\n",
    "\n",
    "bpe_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9ee41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
